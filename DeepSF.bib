Automatically generated by Mendeley Desktop 1.16.3
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Mnih2016,
abstract = {We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task involving finding rewards in random 3D mazes using a visual input.},
archivePrefix = {arXiv},
arxivId = {1602.01783},
author = {Mnih, Volodymyr and Badia, Adri{\`{a}} Puigdom{\`{e}}nech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy P and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
eprint = {1602.01783},
file = {:C$\backslash$:/Users/Ryan/Downloads/1602.01783v2.pdf:pdf},
journal = {arXiv},
pages = {1--28},
pmid = {1000272564},
title = {{Asynchronous Methods for Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1602.01783},
volume = {48},
year = {2016}
}
@article{Cohen2011,
abstract = {Understanding the neurophysiological mechanisms of learning is important for both fundamental and clinical neuroscience. We present a neurophysiologically inspired framework for understanding cortical mechanisms of feedback-guided learning. This framework is based on dynamic changes in systems-level oscillatory synchronization, reflecting changes in synaptic plasticity between stimulus-processing and motor areas that are modulated in a top-down fashion by different areas of the prefrontal cortex. We make new and testable predictions for how large-scale cortical networks support learning from feedback. Testing these predictions may provide new insights into the basic mechanisms underlying learning and how these mechanisms may be impaired in clinical disorders in which feedback learning is compromised. {\textcopyright} 2011 Elsevier Ltd.},
author = {Cohen, Michael X. and Wilmes, Katharina and van de Vijver, Irene},
doi = {10.1016/j.tics.2011.10.004},
file = {:C$\backslash$:/Users/Ryan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cohen, Wilmes, van de Vijver - 2011 - Cortical electrophysiological network dynamics of feedback learning.pdf:pdf},
isbn = {1879-307X (Electronic)$\backslash$r1364-6613 (Linking)},
issn = {13646613},
journal = {Trends in Cognitive Sciences},
number = {12},
pages = {558--566},
pmid = {22078930},
publisher = {Elsevier Ltd},
title = {{Cortical electrophysiological network dynamics of feedback learning}},
url = {http://dx.doi.org/10.1016/j.tics.2011.10.004},
volume = {15},
year = {2011}
}
@article{Mnih2015,
abstract = {The theory of reinforcement learning provides a normative account 1 , deeply rooted in psychological 2 and neuroscientific 3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory pro-cessing systems 4,5 , the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopa-minergic neurons and temporal difference reinforcement learning algorithms 3 . While reinforcement learning agents have achieved some successes in a variety of domains 6–8 , their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks 9–11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games 12},
archivePrefix = {arXiv},
arxivId = {1312.5602},
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei a and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
doi = {10.1038/nature14236},
eprint = {1312.5602},
file = {:C$\backslash$:/Users/Ryan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih et al. - 2015 - Human-level control through deep reinforcement learning.pdf:pdf},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
number = {7540},
pages = {529--533},
pmid = {25719670},
publisher = {Nature Publishing Group},
title = {{Human-level control through deep reinforcement learning}},
url = {http://dx.doi.org/10.1038/nature14236},
volume = {518},
year = {2015}
}
@article{VanHasselt2015,
abstract = {The popular Q-learning algorithm is known to overestimate action values under certain conditions. It was not previously known whether, in practice, such overestimations are common, whether this harms performance, and whether they can generally be prevented. In this paper, we answer all these questions affirmatively. In particular, we first show that the recent DQN algorithm, which combines Q-learning with a deep neural network, suffers from substantial overestimations in some games in the Atari 2600 domain. We then show that the idea behind the Double Q-learning algorithm, which was introduced in a tabular setting, can be generalized to work with large-scale function approximation. We propose a specific adaptation to the DQN algorithm and show that the resulting algorithm not only reduces the observed overestimations, as hypothesized, but that this also leads to much better performance on several games.},
archivePrefix = {arXiv},
arxivId = {1509.06461},
author = {van Hasselt, Hado and Guez, Arthur and Silver, David},
eprint = {1509.06461},
file = {:C$\backslash$:/Users/Ryan/Downloads/1509.06461v3.pdf:pdf},
journal = {arXiv:1509.06461 [cs]},
title = {{Deep Reinforcement Learning with Double Q-learning}},
url = {http://arxiv.org/abs/1509.06461$\backslash$nhttp://www.arxiv.org/pdf/1509.06461.pdf},
year = {2015}
}
@article{Cohen2007,
abstract = {The ability to evaluate outcomes of previous decisions is critical to adaptive decision-making. The feedback-related negativity (FRN) is an event-related potential (ERP) modulation that distinguishes losses from wins, but little is known about the effects of outcome probability on these ERP responses. Further, little is known about the frequency characteristics of feedback processing, for example, event-related oscillations and phase synchronizations. Here, we report an EEG experiment designed to address these issues. Subjects engaged in a probabilistic reinforcement learning task in which we manipulated, across blocks, the probability of winning and losing to each of two possible decision options. Behaviorally, all subjects quickly adapted their decision-making to maximize rewards. ERP analyses revealed that the probability of reward modulated neural responses to wins, but not to losses. This was seen both across blocks as well as within blocks, as learning progressed. Frequency decomposition via complex wavelets revealed that EEG responses to losses, compared to wins, were associated with enhanced power and phase coherence in the theta frequency band. As in the ERP analyses, power and phase coherence values following wins but not losses were modulated by reward probability. Some findings between ERP and frequency analyses diverged, suggesting that these analytic approaches provide complementary insights into neural processing. These findings suggest that the neural mechanisms of feedback processing may differ between wins and losses. ?? 2006 Elsevier Inc. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {NIHMS150003},
author = {Cohen, Michael X. and Elger, Christian E. and Ranganath, Charan},
doi = {10.1016/j.neuroimage.2006.11.056},
eprint = {NIHMS150003},
file = {:C$\backslash$:/Users/Ryan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cohen, Elger, Ranganath - 2007 - Reward expectation modulates feedback-related negativity and EEG spectra.pdf:pdf},
isbn = {1053-8119},
issn = {10538119},
journal = {NeuroImage},
keywords = {Decision-making,EEG oscillations,ERP,Reinforcement learning,Reward prediction error},
number = {2},
pages = {968--978},
pmid = {17257860},
title = {{Reward expectation modulates feedback-related negativity and EEG spectra}},
volume = {35},
year = {2007}
}
@article{Holroyd2002,
abstract = {The authors present a unified account of 2 neural systems concerned with the development and expression of adaptive behaviors: a mesencephalic dopamine system for reinforcement learning and a “generic” error-processing system associated with the anterior cingulate cortex. The existence of the error-processing system has been inferred from the error-related negativity (ERN), a component of the event-related brain potential elicited when human participants commit errors in reaction-time tasks. The authors propose that the ERN is generated when a negative reinforcement learning signal is conveyed to the anterior cingulate cortex via the mesencephalic dopamine system and that this signal is used by the anterior cingulate cortex to modify performance on the task at hand. They provide support for this proposal using both computational modeling and psychophysiological experimentation.},
author = {Holroyd, Clay B and Coles, M. G. H.},
doi = {10.1037//0033-295X.109.4.679},
file = {:C$\backslash$:/Users/Ryan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Holroyd, Coles - 2002 - The neural basis of human error processing Reinforcement learning, dopamine, and the error-related negativity.pdf:pdf},
isbn = {0033-295X (Print)$\backslash$n0033-295X (Linking)},
issn = {0033-295X},
journal = {Psychological Review},
number = {4},
pages = {679 -- 709},
pmid = {12374324},
title = {{The neural basis of human error processing: Reinforcement learning, dopamine, and the error-related negativity.}},
volume = {109},
year = {2002}
}
@book{Sutton1998,
address = {Cambridge, MA, USA},
author = {Sutton, Richard S and Barto, Andrew G},
edition = {1st},
file = {:C$\backslash$:/Users/Ryan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sutton, Barto - 1998 - Introduction to Reinforcement Learning.pdf:pdf},
isbn = {0262193981},
publisher = {MIT Press},
title = {{Introduction to Reinforcement Learning}},
year = {1998}
}
@article{Mnih2013,
author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin A},
file = {:C$\backslash$:/Users/Ryan/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Mnih, Silver, Riedmiller - Unknown - Playing Atari with Deep Reinforcement Learning.pdf:pdf},
journal = {CoRR},
title = {{Playing Atari with Deep Reinforcement Learning}},
url = {http://arxiv.org/abs/1312.5602},
volume = {abs/1312.5},
year = {2013}
}
@article{Gu2016,
abstract = {Model-free reinforcement learning has been suc-cessfully applied to a range of challenging prob-lems, and has recently been extended to han-dle large neural network policies and value func-tions. However, the sample complexity of model-free algorithms, particularly when using high-dimensional function approximators, tends to limit their applicability to physical systems. In this paper, we explore algorithms and repre-sentations to reduce the sample complexity of deep reinforcement learning for continuous con-trol tasks. We propose two complementary tech-niques for improving the efficiency of such algo-rithms. First, we derive a continuous variant of the Q-learning algorithm, which we call normal-ized adantage functions (NAF), as an alternative to the more commonly used policy gradient and actor-critic methods. NAF representation allows us to apply Q-learning with experience replay to continuous tasks, and substantially improves per-formance on a set of simulated robotic control tasks. To further improve the efficiency of our approach, we explore the use of learned models for accelerating model-free reinforcement learn-ing. We show that iteratively refitted local lin-ear models are especially effective for this, and demonstrate substantially faster learning on do-mains where such models are applicable.},
archivePrefix = {arXiv},
arxivId = {1603.00748},
author = {Gu, Shixiang and Lillicrap, Timothy and Sutskever, Ilya and Levine, Sergey and Com, Slevine@google},
eprint = {1603.00748},
file = {:C$\backslash$:/Users/Ryan/Downloads/1603.00748v1.pdf:pdf},
journal = {Icml},
title = {{Continuous Deep Q-Learning with Model-based Acceleration}},
year = {2016}
}
@article{Lillicrap2015,
abstract = {We adapt the ideas underlying the success of Deep Q-Learning to the continuous action domain. We present an actor-critic, model-free algorithm based on the deterministic policy gradient that can operate over continuous action spaces. Using the same learning algorithm, network architecture and hyper-parameters, our algorithm robustly solves more than 20 simulated physics tasks, including classic problems such as cartpole swing-up, dexterous manipulation, legged locomotion and car driving. Our algorithm is able to find policies whose performance is competitive with those found by a planning algorithm with full access to the dynamics of the domain and its derivatives. We further demonstrate that for many of the tasks the algorithm can learn policies end-to-end: directly from raw pixel inputs.},
archivePrefix = {arXiv},
arxivId = {1509.02971},
author = {Lillicrap, Timothy P. and Hunt, Jonathan J. and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
doi = {10.1561/2200000006},
eprint = {1509.02971},
file = {:C$\backslash$:/Users/Ryan/Downloads/1509.02971v5.pdf:pdf},
isbn = {2200000006},
issn = {1935-8237},
journal = {arXiv preprint arXiv:1509.02971},
pages = {1--14},
pmid = {24966830},
title = {{Continuous control with deep reinforcement learning}},
url = {http://arxiv.org/abs/1509.02971},
year = {2015}
}
